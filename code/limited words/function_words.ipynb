{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Module Imports"
      ],
      "metadata": {
        "id": "PNOZIax4R0DC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QuuJYDWdKaXR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from string import digits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data imports and parsing"
      ],
      "metadata": {
        "id": "OLl-C0W0R4BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/10blogs'#a dir of 10 blogs in xml\n",
        "data_xml=[]\n",
        "data_txt={}"
      ],
      "metadata": {
        "id": "msCwPBM5cBTT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#content words\n",
        "content_words=[]\n",
        "with open('/content/drive/MyDrive/research/content_words.txt') as f: #content_words txt file\n",
        "    #Content_list is the list that contains the read lines.     \n",
        "    for line in f:\n",
        "      content_words.append(line.translate({ord(c): None for c in string.whitespace}))\n",
        "    cont = []\n",
        "    for i in range(len(content_words)):\n",
        "      remove_digits = str.maketrans('', '', digits)\n",
        "      cont.append(content_words[i].translate(remove_digits))\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "JaaYR_tvW-Um"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in os.listdir(data_dir):\n",
        "    f = os.path.join(data_dir,filename)\n",
        "    if os.path.isfile(f):\n",
        "          data_xml.append(f)"
      ],
      "metadata": {
        "id": "iGRbfhqxcRTL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data_xml:\n",
        "  tree = ET.parse(i)  #parsing\n",
        "  root = tree.getroot()   #parsing\n",
        "  textArr = []\n",
        "  for child in root:\n",
        "      if child.tag=='post':\n",
        "        textArr.append(str(child.text))\n",
        "        data_txt[i]=textArr\n",
        "      \n"
      ],
      "metadata": {
        "id": "zHit9FYB7Wl9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = list(data_txt.values())\n",
        "author = list(data_txt.keys())\n",
        "\n",
        "for i in range(len(texts)):\n",
        "  texts[i] = str(texts[i])"
      ],
      "metadata": {
        "id": "a6ntGe7v-wu-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limiting number of entries and characters inside each blogs\n"
      ],
      "metadata": {
        "id": "fhJhsat-qlVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(texts)):\n",
        "  string1= texts[i][:len(texts[i])//2]\n",
        "  string2= texts[i][len(texts[i])//2:]\n",
        "  texts[i] = [string1[:1500], string2[:1500]]"
      ],
      "metadata": {
        "id": "POPOoCnQqzYj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_author = []\n",
        "for i in range(10):\n",
        "  fixed_author.append(author[i].strip('/content/drive/MyDrive/10blogs/'))\n",
        "  "
      ],
      "metadata": {
        "id": "MkoNwna2q0dF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(texts)):\n",
        "\n",
        "  print(len(texts[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilAv9CTSq3cO",
        "outputId": "a5c1d7e3-589f-46ff-c760-7e000bbc704a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a data frame"
      ],
      "metadata": {
        "id": "ri-kdl5nTpJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'texts':texts,\n",
        "                   'author':fixed_author})\n",
        "df=df.explode('texts')"
      ],
      "metadata": {
        "id": "Hs0r1VdMGsbl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xfeature = df['texts']\n",
        "ylabel = df['author']"
      ],
      "metadata": {
        "id": "O2yIhP28kVl1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of documents\n",
        "docs = xfeature\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer(vocabulary=cont)\n",
        "# tokenize and build vocab\n",
        "X = vectorizer.fit_transform(docs)\n",
        "print('vocabulary: ', vectorizer.vocabulary_)\n",
        "# summarize encoded vector\n",
        "print('shape: ', X.shape)\n",
        "print('vectors: ', X.toarray())"
      ],
      "metadata": {
        "id": "s7l8y_NyNJ_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d721709c-cfb5-4c0b-c9ad-9d6516f2d8b5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary:  {'a': 0, 'about': 1, 'above': 2, 'across': 3, 'after': 4, 'afterwards': 5, 'again': 6, 'against': 7, 'all': 8, 'almost': 9, 'alone': 10, 'along': 11, 'already': 12, 'also': 13, 'although': 14, 'always': 15, 'am': 16, 'among': 17, 'amongst': 18, 'amoungst': 19, 'an': 20, 'and': 21, 'another': 22, 'any': 23, 'anyhow': 24, 'anyone': 25, 'anything': 26, 'anyway': 27, 'anywhere': 28, 'are': 29, 'around': 30, 'as': 31, 'at': 32, 'be': 33, 'became': 34, 'because': 35, 'been': 36, 'before': 37, 'beforehand': 38, 'behind': 39, 'being': 40, 'below': 41, 'beside': 42, 'besides': 43, 'between': 44, 'beyond': 45, 'both': 46, 'but': 47, 'by': 48, 'can': 49, 'cannot': 50, 'could': 51, 'dare': 52, 'despite': 53, 'did': 54, 'do': 55, 'does': 56, 'done': 57, 'down': 58, 'during': 59, 'each': 60, 'eg': 61, 'either': 62, 'else': 63, 'elsewhere': 64, 'enough': 65, 'etc': 66, 'even': 67, 'ever': 68, 'every': 69, 'everyone': 70, 'everything': 71, 'everywhere': 72, 'except': 73, 'few': 74, 'first': 75, 'for': 76, 'former': 77, 'formerly': 78, 'from': 79, 'further': 80, 'furthermore': 81, 'had': 82, 'has': 83, 'have': 84, 'he': 85, 'hence': 86, 'her': 87, 'here': 88, 'hereabouts': 89, 'hereafter': 90, 'hereby': 91, 'herein': 92, 'hereinafter': 93, 'heretofore': 94, 'hereunder': 95, 'hereupon': 96, 'herewith': 97, 'hers': 98, 'herself': 99, 'him': 100, 'himself': 101, 'his': 102, 'how': 103, 'however': 104, 'i': 105, 'ie': 106, 'if': 107, 'in': 108, 'indeed': 109, 'inside': 110, 'instead': 111, 'into': 112, 'is': 113, 'it': 114, 'its': 115, 'itself': 116, 'last': 117, 'latter': 118, 'latterly': 119, 'least': 120, 'less': 121, 'lot': 122, 'lots': 123, 'many': 124, 'may': 125, 'me': 126, 'meanwhile': 127, 'might': 128, 'mine': 129, 'more': 130, 'moreover': 131, 'most': 132, 'mostly': 133, 'much': 134, 'must': 135, 'my': 136, 'myself': 137, 'namely': 138, 'near': 139, 'need': 140, 'neither': 141, 'never': 142, 'nevertheless': 143, 'next': 144, 'no': 145, 'nobody': 146, 'none': 147, 'noone': 148, 'nor': 149, 'not': 150, 'nothing': 151, 'now': 152, 'nowhere': 153, 'of': 154, 'off': 155, 'often': 156, 'oftentimes': 157, 'on': 158, 'once': 159, 'one': 160, 'only': 161, 'onto': 162, 'or': 163, 'other': 164, 'others': 165, 'otherwise': 166, 'ought': 167, 'our': 168, 'ours': 169, 'ourselves': 170, 'out': 171, 'outside': 172, 'over': 173, 'per': 174, 'perhaps': 175, 'rather': 176, 're': 177, 'same': 178, 'second': 179, 'several': 180, 'shall': 181, 'she': 182, 'should': 183, 'since': 184, 'so': 185, 'some': 186, 'somehow': 187, 'someone': 188, 'something': 189, 'sometime': 190, 'sometimes': 191, 'somewhat': 192, 'somewhere': 193, 'still': 194, 'such': 195, 'than': 196, 'that': 197, 'the': 198, 'their': 199, 'theirs': 200, 'them': 201, 'themselves': 202, 'then': 203, 'thence': 204, 'there': 205, 'thereabouts': 206, 'thereafter': 207, 'thereby': 208, 'therefore': 209, 'therein': 210, 'thereof': 211, 'thereon': 212, 'thereupon': 213, 'these': 214, 'they': 215, 'third': 216, 'this': 217, 'those': 218, 'though': 219, 'through': 220, 'throughout': 221, 'thru': 222, 'thus': 223, 'to': 224, 'together': 225, 'too': 226, 'top': 227, 'toward': 228, 'towards': 229, 'under': 230, 'until': 231, 'up': 232, 'upon': 233, 'us': 234, 'used': 235, 'very': 236, 'via': 237, 'was': 238, 'we': 239, 'well': 240, 'were': 241, 'what': 242, 'whatever': 243, 'when': 244, 'whence': 245, 'whenever': 246, 'where': 247, 'whereafter': 248, 'whereas': 249, 'whereby': 250, 'wherein': 251, 'whereupon': 252, 'wherever': 253, 'whether': 254, 'which': 255, 'while': 256, 'whither': 257, 'who': 258, 'whoever': 259, 'whole': 260, 'whom': 261, 'whose': 262, 'why': 263, 'whyever': 264, 'will': 265, 'with': 266, 'within': 267, 'without': 268, 'would': 269, 'yes': 270, 'yet': 271, 'you': 272, 'your': 273, 'yours': 274, 'yourself': 275, 'yourselves': 276, '': 277}\n",
            "shape:  (20, 278)\n",
            "vectors:  [[0 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X, ylabel, train_size=0.10, random_state=42)\n",
        "                                            "
      ],
      "metadata": {
        "id": "ZprsY4cyJeMn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d6wEyMak8iW",
        "outputId": "2eb644f4-f6f6-487b-bf99-8a2fc1dbf673"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 278)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tests"
      ],
      "metadata": {
        "id": "T1O2PUGQTv7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MultinomialNB() #classification\n",
        "clf.fit(X_train , y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR44rptElXpt",
        "outputId": "66918352-de9e-4315-b523-8b02832f5df0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "print(accuracy_score(y_test,clf.predict(X_test)))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyRsfgK4mZd-",
        "outputId": "f3bc9431-e3d3-4861-e4b4-9de3124faba8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1111111111111111\n",
            "                                          precision    recall  f1-score   support\n",
            "\n",
            "     2000257.female.14.indUnk.Scorpio.xm       0.00      0.00      0.00         2\n",
            "29253.female.26.HumanResources.Cancer.xm       0.00      0.00      0.00         2\n",
            "          37626.male.24.indUnk.Gemini.xm       0.20      1.00      0.33         1\n",
            "       40964.female.23.RealEstate.Leo.xm       0.08      1.00      0.14         1\n",
            "      43781.female.25.Education.Aries.xm       0.00      0.00      0.00         2\n",
            " 48638.male.36.Technology.Sagittarius.xm       0.00      0.00      0.00         2\n",
            "          566970.male.27.indUnk.Libra.xm       0.00      0.00      0.00         2\n",
            "   61176.male.33.Technology.Capricorn.xm       0.00      0.00      0.00         2\n",
            "         67459.male.34.Arts.Capricorn.xm       0.00      0.00      0.00         2\n",
            "             854.male.25.indUnk.Virgo.xm       0.00      0.00      0.00         2\n",
            "\n",
            "                                accuracy                           0.11        18\n",
            "                               macro avg       0.03      0.20      0.05        18\n",
            "                            weighted avg       0.02      0.11      0.03        18\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit = LogisticRegression() #logistic regression\n",
        "logit.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R35CV-LipdyG",
        "outputId": "69b247ff-1567-45c8-f548-988f94ebaf09"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test,logit.predict(X_test))"
      ],
      "metadata": {
        "id": "5sCYypeYRMED",
        "outputId": "df921f3d-4f3b-428b-a3b6-68a607803382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1111111111111111"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f26hrcancer = [\"Billy Zane  and Jaoquin Phoenix are half-brothers.  Their father may or may not be  the  Lazarus (he could just be  a  Lazarus- type ), this mystical paternity has granted them the ability to raise the recently deceased from the dead.    Do they have a mission?  Yes, to find out the fate of their father and, if he still lives, kill him and set him free from the bonds of the Earth because for every year the Dad is alive, a single soul must take his place in the afterlife.  A soul that would have otherwise continued to live.  Oh, the angst!  Are the brothers raising the right people from the dead? Or are they making things worse?  Would they be better off doing nothing, or not bringing people back and just killing Dad?  How do they find all of this out?  Does a little man who lives in a cave and wears a white robe with an equally white beard tell them?  No!  Their mothers belonged to a cult - both died in childbirth, of course - and they were raised by a woman who amounts to the cult's den mother (because even cults need someone to clean the bathrooms).    Is there a conflict between the brothers? Of course!  Jaoquin's character is afraid of what will be revealed and would rather turn back than continue on and Billy's character drags him forward by sheer inertia.    But who will play Dad?  That is the question.  \"]\n",
        "vect1 = vectorizer.transform(f26hrcancer).toarray()\n",
        "\n",
        "m25virgo = [\"For someone new to London (or just visiting as I have in the past) the London Underground is a saviour (despite being hot, crowded and late). Get a travelcard and you can just from one place to another without any problem at all.    But the problem with navigating London via the tube is that it gives a totally skewed impression of the Geography of the place. People get the tube when walking would have taken half the time, if only they knew where they were going. For example, in the past I've gone from Tottenham Court Road to Covent Garden by getting the Northern line to Leicester Square and then changing on to the Picadilly line. Now, that probably took about 25 minutes, but if I'd just walkd I could have done it in 15. Ish.         A couple of weeks ago I bought an A-Z which has put a few things into perspective, but for the most part I'm still stuck on tube-geography. Yesterday I came across a  urlLink handy little web page  which gives a number of tube maps, including a  urlLink geographically correct map of the underground .  Nice to see, but doesn't really give you an impression of how you can get from place to another.    But also included on that page is a PDF of the  urlLink geographical map superimprosed onto a street map ! It shows landmarks, train stations and railway lines, tube stop and lines, all in the correct locations, so now you really can see just how quick it would have been to walk instead of fighting the commute! \"]\n",
        "vect2 = vectorizer.transform(m25virgo).toarray()\n",
        "\n",
        "m36techsagi =[\"Microsoft, it's long been known, has the digital music market (refined and defined by Apple) in its sights. Looks like they're getting close enough to the initiation of their plans that they're willing to start talking trash: \"]\n",
        "vect3 = vectorizer.transform(m36techsagi).toarray()\n"
      ],
      "metadata": {
        "id": "4LUXIqX1p5Ul"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(vect2)"
      ],
      "metadata": {
        "id": "aR_MXWEYQTLC",
        "outputId": "f91bf56f-7f50-46e3-f368-1993165de1b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['40964.female.23.RealEstate.Leo.xm'], dtype='<U33')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit.predict(vect2)"
      ],
      "metadata": {
        "id": "76Xx2i6LQcGm",
        "outputId": "d81841aa-a25a-4896-9239-fedd19646f61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['40964.female.23.RealEstate.Leo.xm'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}
